server:
  port: 8090
debug: false
logging.level.org.springframework.boot.autoconfigure:  debug
logging.level.com.roncoo.eshop.mapper: debug
spring:
  datasource:
    url: jdbc:mysql://196.128.133.130:3306/eshop?serverTimezone=UTC&useUnicode=true&characterEncoding=utf-8&useSSL=false
    username: root
    password: 123456
    driver-class-name: com.mysql.jdbc.Driver
  #redis集群
  redis:
    cluster:
      nodes:
        - 192.168.133.133:7001
        - 192.168.133.133:7002
        - 192.168.133.128:7003
        - 192.168.133.128:7004
        - 192.168.133.129:7005
        - 192.168.133.129:7006
    database: 0
    timeout: 1000
  # 本地缓存
  cache:
    type: ehcache
    ehcache:
      config: classpath:/ehcache.xml

  kafka:
    ###########【指定 kafka 代理地址，可以多个】###########
    bootstrap-servers: http://192.168.133.133:9092,http://192.168.133.128:9092,http://192.168.133.129:9092
    ###########【初始化生产者配置】###########
    producer:
      # 重试次数，默认Integer.MAX_VALUE
      retries: 3
      # ack应答机制，默认1，即只需要确认leader收到消息
      acks: all
      # 同一批次内存大小（默认16K）
      batch-size: 16384
      # 生产者内存缓存区大小(32M)
      buffer-memory: 33554432
      # key和value的序列化（默认为String类型，可以不设置）
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      # 如果发送的消息体是一个对象，对象需要序列化
      value-serializer: com.zhan.eshop.cache.serializable.KafkaMessageSerializable
      #properties:
        # 使用自定义的分区选择器
        #{partitioner.class: com.zhan.kafka.MyPartition, acks: all}
    ###########【初始化消费者配置】###########
    consumer:
      # 默认的消费组ID
      group-id: eshop-cache-group
      # 当kafka中没有初始offset或offset超出范围时将自动重置offset
      # earliest:重置为分区中最小的offset;
      # latest:重置为分区中最新的offset(消费分区中新产生的数据);
      # none:只要有一个分区不存在已提交的offset,就抛出异常;
      auto-offset-reset: earliest
      # 是否自动提交offset
      enable-auto-commit: false
      # key和value的反序列化（默认为String类型，可以不设置）
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      # 如果消费的消息体是一个对象，对象需要反序列化
      value-deserializer: com.zhan.eshop.cache.serializable.KafkaMessageDeserializer
      max-poll-records: 20000
    #监听
    listener:
      # 并发能力
      concurrency: 3
      # 设置手动提交的时候，需要设置ackMode
      ack-mode: MANUAL
      # 消费端监听的topic不存在时，项目启动会报错(关掉)
      missing-topics-fatal: false

#topic, 也可以在代码中发送消息时写：kafkaTemplate.send("testTopic","message");
kafka.eshop.topic: test_topic
kafka.eshop.cacheTopic: eshop-cache




